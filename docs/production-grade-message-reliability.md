# Production-Grade Message Reliability Patterns

## Overview
This document outlines production-grade patterns used by companies like Netflix, Uber, Stripe, and Discord to handle missed messages and ensure reliability in real-time systems with millions of users.

---

## 1. **Idempotency & Deduplication**

### Current Implementation ✅
- **Dedupe Keys**: Messages use `dedupeKey` field (e.g., `tempId` or `${roomId}-${senderId}-${timestamp}`)
- **Client-side deduplication**: Chat store prevents duplicate messages within 5-second window
- **Database indexes**: `dedupeKey` is indexed for fast lookups

### Production Enhancements Needed

#### A. **Idempotency Tokens (Recommended)**
```typescript
// Enhanced message schema with idempotency
interface MessageEvent {
  id: string;
  idempotencyKey: string; // UUID generated by client
  dedupeKey: string; // For backward compatibility
  correlationId: string; // For tracing across services
  // ... rest of fields
}

// In listener:
async onMessage(data: MessageEvent['data']) {
  // Check if already processed
  const existing = await Message.findOne({ 
    idempotencyKey: data.idempotencyKey 
  });
  
  if (existing) {
    console.log(`[Idempotency] Message ${data.idempotencyKey} already processed`);
    await this.ack();
    return; // Safe to skip - already processed
  }
  
  // Process message...
  await message.save();
}
```

#### B. **Distributed Deduplication Cache (Redis)**
```typescript
// Use Redis for fast deduplication checks (TTL: 24 hours)
const idempotencyKey = `msg:${data.idempotencyKey}`;
const exists = await redis.get(idempotencyKey);
if (exists) {
  return; // Already processed
}
await redis.setex(idempotencyKey, 86400, 'processed'); // 24h TTL
```

**Companies using this**: Stripe (idempotency keys), AWS (idempotency tokens), PayPal

---

## 2. **Dead Letter Queues (DLQ)**

### Problem
Messages that fail repeatedly (e.g., invalid data, downstream service down) can cause infinite retry loops.

### Solution: DLQ Pattern

```typescript
// Enhanced baseListener with DLQ support
export abstract class Listener<T extends BaseEvent> {
  private retryCount: Map<string, number> = new Map();
  private readonly maxRetries = 5;
  private readonly dlqPublisher?: DLQPublisher;

  async onMessage(data: T['data'], payload: EachMessagePayload) {
    const messageId = payload.message.offset.toString();
    const currentRetries = this.retryCount.get(messageId) || 0;

    try {
      await this.onMessage(data, payload);
      this.retryCount.delete(messageId); // Success - clear retry count
      await this.ack(payload);
    } catch (error) {
      if (currentRetries >= this.maxRetries) {
        // Move to DLQ
        await this.dlqPublisher?.publish({
          originalTopic: this.topic,
          originalOffset: payload.message.offset,
          originalData: data,
          error: error.message,
          retryCount: currentRetries,
          timestamp: new Date().toISOString(),
        });
        await this.ack(payload); // Acknowledge to prevent infinite retry
        console.error(`[DLQ] Moved message to DLQ after ${currentRetries} retries`);
      } else {
        // Increment retry count and let Kafka redeliver
        this.retryCount.set(messageId, currentRetries + 1);
        throw error; // Don't ack - let Kafka redeliver
      }
    }
  }
}
```

**Companies using this**: Netflix, Uber, LinkedIn, AWS SQS

---

## 3. **Outbox Pattern (Transactional Publishing)**

### Problem
Database transaction succeeds but Kafka publish fails → data inconsistency.

### Solution: Outbox Table

```typescript
// 1. Save to outbox table in same transaction
await mongoose.connection.transaction(async (session) => {
  // Save message
  await message.save({ session });
  
  // Save event to outbox
  await OutboxEvent.create([{
    eventType: 'message.created',
    eventData: messageData,
    status: 'pending',
    createdAt: new Date(),
  }], { session });
  
  // Transaction commits atomically
});

// 2. Separate worker polls outbox and publishes
setInterval(async () => {
  const pending = await OutboxEvent.find({ status: 'pending' })
    .limit(100)
    .sort({ createdAt: 1 });
  
  for (const event of pending) {
    try {
      await publisher.publish(event.eventData);
      event.status = 'published';
      event.publishedAt = new Date();
      await event.save();
    } catch (error) {
      event.retryCount = (event.retryCount || 0) + 1;
      if (event.retryCount > 5) {
        event.status = 'failed';
      }
      await event.save();
    }
  }
}, 1000); // Poll every second
```

**Companies using this**: Microsoft, Amazon, EventStore

---

## 4. **Consumer Offset Management**

### Current Implementation ✅
- `autoCommit: false` - Manual commits only
- `fromBeginning: true` for critical events (RoomCreated, AgentIngested)
- Session timeout: 30s, Heartbeat: 3s

### Production Enhancements

#### A. **Offset Checkpointing**
```typescript
// Periodically checkpoint offsets to external store (Redis/DB)
setInterval(async () => {
  const offsets = await consumer.committedOffsets();
  await redis.set(`offsets:${groupId}:${topic}`, JSON.stringify(offsets));
}, 5000); // Every 5 seconds

// On restart, resume from checkpoint
const savedOffsets = await redis.get(`offsets:${groupId}:${topic}`);
if (savedOffsets) {
  await consumer.seek({ topic, partition: 0, offset: savedOffsets[0] });
}
```

#### B. **Consumer Lag Monitoring**
```typescript
// Monitor consumer lag (messages behind)
const lag = await consumer.describeGroup(groupId);
const lagMetrics = lag.members.map(m => ({
  memberId: m.memberId,
  lag: m.lag, // Messages behind
}));

// Alert if lag > threshold
if (lagMetrics.some(m => m.lag > 1000)) {
  await alertingService.sendAlert('High consumer lag detected');
}
```

**Companies using this**: LinkedIn (Burrow), Netflix (Chaperone), Uber

---

## 5. **Distributed Tracing & Observability**

### Current State
- Basic console logging
- No distributed tracing
- Limited metrics

### Production Requirements

#### A. **OpenTelemetry Integration**
```typescript
import { trace, context } from '@opentelemetry/api';

async onMessage(data: T['data'], payload: EachMessagePayload) {
  const span = trace.getActiveSpan();
  span?.setAttributes({
    'kafka.topic': this.topic,
    'kafka.partition': payload.partition,
    'kafka.offset': payload.message.offset,
    'message.id': data.id,
  });
  
  try {
    await this.processMessage(data);
    span?.setStatus({ code: SpanStatusCode.OK });
  } catch (error) {
    span?.setStatus({ code: SpanStatusCode.ERROR, message: error.message });
    span?.recordException(error);
    throw error;
  }
}
```

#### B. **Metrics (Prometheus)**
```typescript
import { Counter, Histogram } from 'prom-client';

const messageProcessed = new Counter({
  name: 'kafka_messages_processed_total',
  help: 'Total messages processed',
  labelNames: ['topic', 'status'],
});

const processingTime = new Histogram({
  name: 'kafka_message_processing_seconds',
  help: 'Message processing time',
  labelNames: ['topic'],
});

async onMessage(data: T['data']) {
  const timer = processingTime.startTimer({ topic: this.topic });
  try {
    await this.processMessage(data);
    messageProcessed.inc({ topic: this.topic, status: 'success' });
  } catch (error) {
    messageProcessed.inc({ topic: this.topic, status: 'error' });
    throw error;
  } finally {
    timer();
  }
}
```

**Companies using this**: Google (Dapper), Twitter (Zipkin), Uber (Jaeger), Netflix

---

## 6. **Circuit Breaker Pattern**

### Problem
If downstream service is down, continuous retries waste resources and delay recovery.

### Solution
```typescript
import { CircuitBreaker } from 'opossum';

const breaker = new CircuitBreaker(async (data) => {
  return await downstreamService.process(data);
}, {
  timeout: 5000,
  errorThresholdPercentage: 50,
  resetTimeout: 30000, // 30s before retry
});

// In listener
async onMessage(data: T['data']) {
  try {
    await breaker.fire(data);
  } catch (error) {
    if (breaker.opened) {
      // Circuit is open - service is down
      // Option 1: Move to DLQ
      // Option 2: Store locally and retry later
      await this.storeForRetry(data);
      await this.ack(); // Don't block Kafka
    } else {
      throw error; // Transient error - retry
    }
  }
}
```

**Companies using this**: Netflix (Hystrix), Microsoft, Amazon

---

## 7. **Saga Pattern (Distributed Transactions)**

### Problem
Multi-step operations (e.g., create room → add participants → send notifications) can fail partially.

### Solution: Event Sourcing + Saga
```typescript
// Saga orchestrator
class RoomCreationSaga {
  async execute(roomData: RoomData) {
    const sagaId = uuid();
    
    try {
      // Step 1: Create room
      const room = await this.createRoom(roomData);
      await this.recordStep(sagaId, 'room.created', room.id);
      
      // Step 2: Add participants
      await this.addParticipants(room.id, roomData.participants);
      await this.recordStep(sagaId, 'participants.added', room.id);
      
      // Step 3: Send notifications
      await this.sendNotifications(room.id);
      await this.recordStep(sagaId, 'notifications.sent', room.id);
      
      await this.completeSaga(sagaId);
    } catch (error) {
      // Compensate (rollback)
      await this.compensate(sagaId);
      throw error;
    }
  }
  
  async compensate(sagaId: string) {
    const steps = await this.getSagaSteps(sagaId);
    
    // Reverse in reverse order
    for (const step of steps.reverse()) {
      if (step.name === 'room.created') {
        await this.deleteRoom(step.data);
      } else if (step.name === 'participants.added') {
        await this.removeParticipants(step.data);
      }
      // ... other compensations
    }
  }
}
```

**Companies using this**: Uber, Airbnb, Amazon

---

## 8. **Health Checks & Readiness Probes**

### Current State
- Basic health endpoints
- No Kafka consumer health checks

### Production Requirements

```typescript
// Health check endpoint
app.get('/health', async (req, res) => {
  const health = {
    status: 'healthy',
    checks: {
      mongodb: await checkMongoDB(),
      kafka: await checkKafka(),
      redis: await checkRedis(),
      consumers: await checkConsumers(),
    },
  };
  
  const allHealthy = Object.values(health.checks).every(c => c.status === 'healthy');
  res.status(allHealthy ? 200 : 503).json(health);
});

async function checkConsumers() {
  const consumers = [
    { name: 'message-ingest', groupId: 'chat-service-message-ingest' },
    { name: 'room-created', groupId: 'chat-service-room-created' },
    // ... other consumers
  ];
  
  const results = await Promise.all(
    consumers.map(async (c) => {
      const lag = await getConsumerLag(c.groupId, c.topic);
      return {
        name: c.name,
        status: lag < 1000 ? 'healthy' : 'degraded',
        lag,
      };
    })
  );
  
  return {
    status: results.every(r => r.status === 'healthy') ? 'healthy' : 'degraded',
    consumers: results,
  };
}
```

**Kubernetes Readiness Probe**:
```yaml
readinessProbe:
  httpGet:
    path: /health
    port: 3000
  initialDelaySeconds: 10
  periodSeconds: 5
  failureThreshold: 3
```

---

## 9. **Message Ordering Guarantees**

### Current State
- No explicit ordering guarantees
- Messages can be processed out of order

### Production Solution: Partition Keys

```typescript
// In publisher
async publish(data: T['data']) {
  const partitionKey = data.roomId; // Same room → same partition → ordered
  
  await producer.send({
    topic: this.topic,
    messages: [{
      key: partitionKey, // Critical for ordering
      value: JSON.stringify(data),
    }],
  });
}

// In consumer
// maxInFlightRequests: 1 ensures messages in same partition are processed sequentially
consumer({
  groupId,
  maxInFlightRequests: 1, // Process one at a time per partition
});
```

**Trade-off**: Ordering reduces throughput. Use only when necessary (e.g., room messages, user state changes).

**Companies using this**: Discord (per-channel ordering), Slack, WhatsApp

---

## 10. **Chaos Engineering & Testing**

### Production Practices

#### A. **Chaos Monkey (Netflix)**
```typescript
// Randomly kill pods to test resilience
if (Math.random() < 0.01) { // 1% chance
  process.exit(1); // Simulate crash
}
```

#### B. **Message Replay Testing**
```typescript
// Replay messages from a specific offset
async function replayMessages(topic: string, fromOffset: number) {
  const consumer = kafka.consumer({ groupId: 'replay-group' });
  await consumer.subscribe({ topic, fromBeginning: false });
  
  await consumer.run({
    eachMessage: async ({ message, partition }) => {
      if (parseInt(message.offset) >= fromOffset) {
        await processMessage(message);
      }
    },
  });
}
```

#### C. **Load Testing**
- Use tools like **k6**, **Gatling**, or **JMeter**
- Test with realistic message volumes (e.g., 10K messages/sec)
- Monitor consumer lag, processing time, error rates

---

## 11. **Recommended Implementation Roadmap**

### Phase 1: Critical (Immediate)
1. ✅ **Idempotency checks** in all listeners (use `dedupeKey`)
2. ✅ **DLQ implementation** for failed messages
3. ✅ **Consumer lag monitoring** and alerts
4. ✅ **Health checks** with Kafka consumer status

### Phase 2: High Priority (Next Sprint)
5. **Distributed tracing** (OpenTelemetry)
6. **Metrics** (Prometheus + Grafana)
7. **Outbox pattern** for critical events (room creation, payments)
8. **Circuit breakers** for external API calls

### Phase 3: Scale (Future)
9. **Saga pattern** for complex workflows
10. **Message ordering** where needed (room messages)
11. **Chaos engineering** tests
12. **Automated message replay** for recovery

---

## 12. **Monitoring Dashboard (Grafana)**

### Key Metrics to Track

1. **Consumer Lag** (per topic, per consumer group)
   - Alert if lag > 1000 messages
   - Alert if lag increasing over time

2. **Message Processing Rate**
   - Messages processed per second
   - Success vs. error rate

3. **Processing Time** (P50, P95, P99)
   - Alert if P95 > 1 second

4. **DLQ Size**
   - Alert if DLQ has messages

5. **Consumer Group Health**
   - Number of active consumers
   - Rebalancing frequency

6. **Error Rate by Topic**
   - Track errors per topic
   - Alert if error rate > 1%

---

## 13. **Real-World Examples**

### Netflix
- **Pattern**: Event-driven architecture with Kafka
- **Key**: Idempotency, DLQ, extensive monitoring
- **Scale**: Billions of events per day

### Uber
- **Pattern**: Event sourcing + CQRS
- **Key**: Saga pattern for ride matching, circuit breakers
- **Scale**: Millions of rides per day

### Discord
- **Pattern**: Per-channel message ordering
- **Key**: Partition keys for ordering, consumer groups
- **Scale**: Billions of messages per day

### Stripe
- **Pattern**: Idempotency keys for all API calls
- **Key**: Idempotency tokens, outbox pattern
- **Scale**: Millions of transactions per day

---

## 14. **Best Practices Summary**

1. **Always make handlers idempotent** - Use dedupe keys or idempotency tokens
2. **Never lose messages** - Use DLQ for failed messages, manual commits
3. **Monitor everything** - Consumer lag, error rates, processing time
4. **Test failure scenarios** - Chaos engineering, load testing
5. **Design for eventual consistency** - Don't assume immediate consistency
6. **Use correlation IDs** - Trace messages across services
7. **Implement health checks** - Know when services are unhealthy
8. **Set up alerts** - Proactive monitoring beats reactive debugging

---

## Conclusion

Production-grade message reliability requires:
- **Idempotency** (handle duplicates safely)
- **DLQ** (prevent infinite retries)
- **Monitoring** (know when things break)
- **Testing** (verify resilience)
- **Observability** (trace and debug issues)

Start with Phase 1 (idempotency, DLQ, monitoring) and gradually add more sophisticated patterns as you scale.

